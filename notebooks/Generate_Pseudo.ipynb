{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses [this blog post](https://medium.com/cloud-to-street/jumpstart-your-machine-learning-satellite-competition-submission-2443b40d0a5a) and [this video](https://www.youtube.com/watch?v=SsnWM1xWDu4) as references."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mz4KCyB4Pw6Q"
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from glob import glob\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import ttach as tta\n",
    "import torch.nn as nn\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N1QXI66oQ0z_"
   },
   "source": [
    "## Set up paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SOi7wYOKQyjF",
    "outputId": "916e5607-3933-41d5-fefb-70b0a1e7814b"
   },
   "outputs": [],
   "source": [
    "# path to dataset root directory\n",
    "dset_root = '/home/jupyter/Flood_Comp/'\n",
    "\n",
    "test_dir = os.path.join(dset_root, 'test_internal')\n",
    "n_test_regions = len(glob(test_dir+'/*/'))\n",
    "print('Number of test temporal-regions: {}'.format(n_test_regions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HWNe2TxWVrQ1"
   },
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lHnXysQzVtr_"
   },
   "outputs": [],
   "source": [
    "def get_test_id(path):\n",
    "    return path.split(\"_\")[0] + \"_\" + path.split(\"_\")[1]\n",
    "\n",
    "def make_im_name(id, suffix):\n",
    "    return id.split(\".\")[0] + f\"_{suffix}.png\"\n",
    "\n",
    "def s1_to_rgb(vv_image, vh_image):\n",
    "    ratio_image = np.clip(np.nan_to_num(vh_image/vv_image, 0), 0, 1)\n",
    "    rgb_image = np.stack((vv_image, vh_image, 1-ratio_image), axis=2)\n",
    "    return rgb_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kMhA2NnmQ5-G"
   },
   "source": [
    "## Create dataframe\n",
    "\n",
    "As per [the competition website](https://nasa-impact.github.io/etci2021/), the submission file needs to be generated following a particular sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://git.io/JsRTE -O test_sentinel.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B2oExzdPiPfR",
    "outputId": "d955d599-b1be-485c-88c0-214444e10086"
   },
   "outputs": [],
   "source": [
    "test_file_sequence = pd.read_csv(\"test_sentinel.csv\", header=None)\n",
    "test_file_sequence = test_file_sequence.values.squeeze().tolist()\n",
    "\n",
    "all_test_vv = [os.path.join(test_dir, get_test_id(id), \"tiles\", \"vv\", make_im_name(id, \"vv\")) \n",
    "                                                                for id in test_file_sequence]\n",
    "all_test_vh = [os.path.join(test_dir, get_test_id(id), \"tiles\", \"vh\", make_im_name(id, \"vh\")) \n",
    "                                                                for id in test_file_sequence]\n",
    "\n",
    "paths = {'vv_image_path': all_test_vv,\n",
    "         'vh_image_path': all_test_vh,\n",
    "}\n",
    "\n",
    "test_df = pd.DataFrame(paths)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BW5yZ_I0RFK6"
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gus4cgUzdx3o"
   },
   "outputs": [],
   "source": [
    "class ETCIDataset(Dataset):\n",
    "    def __init__(self, dataframe, split, transform=None):\n",
    "        self.split = split\n",
    "        self.dataset = dataframe\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset.shape[0]\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        example = {}\n",
    "        \n",
    "        df_row = self.dataset.iloc[index]\n",
    "\n",
    "        # load vv and vh images\n",
    "        vv_image = cv2.imread(df_row['vv_image_path'], 0) / 255.0\n",
    "        vh_image = cv2.imread(df_row['vh_image_path'], 0) / 255.0\n",
    "        \n",
    "        # convert vv and ch images to rgb\n",
    "        rgb_image = s1_to_rgb(vv_image, vh_image)\n",
    "\n",
    "        if self.split == 'test':\n",
    "            # no flood mask should be available\n",
    "            example['image'] = rgb_image.transpose((2,0,1)).astype('float32')\n",
    "            example['vv_image_path'] = df_row['vv_image_path']\n",
    "            example['vh_image_path'] = df_row['vh_image_path']\n",
    "        else:\n",
    "            # load ground truth flood mask\n",
    "            flood_mask = cv2.imread(df_row['flood_label_path'], 0) / 255.0\n",
    "\n",
    "            # compute transformations\n",
    "            if self.transform:\n",
    "                augmented = self.transform(image=rgb_image, mask=flood_mask)\n",
    "                rgb_image = augmented['image']\n",
    "                flood_mask = augmented['mask']\n",
    "\n",
    "            example['image'] = rgb_image.transpose((2,0,1)).astype('float32')\n",
    "            example['mask'] = flood_mask.astype('int64')\n",
    "\n",
    "        return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ls9VrVu1d3Ba"
   },
   "outputs": [],
   "source": [
    "test_dataset = ETCIDataset(test_df, split='test', transform=None)\n",
    "\n",
    "batch_size = 96 * torch.cuda.device_count()\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, \n",
    "                         num_workers=os.cpu_count(), pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uc2z3thWRG5Q"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembling and pseudo labeling\n",
    "\n",
    "We start by defining the model classes and the paths to their pre-trained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_mobilenet = smp.Unet(\n",
    "    encoder_name=\"mobilenet_v2\", \n",
    "    encoder_weights=None, \n",
    "    in_channels=3,                  \n",
    "    classes=2                      \n",
    ")\n",
    "\n",
    "upp_mobilenet = smp.UnetPlusPlus(\n",
    "    encoder_name=\"mobilenet_v2\", \n",
    "    encoder_weights=None, \n",
    "    in_channels=3,                  \n",
    "    classes=2                      \n",
    ")\n",
    "\n",
    "model_defs = [unet_mobilenet, upp_mobilenet]\n",
    "model_paths = [\"Best_IoU/unet_mobilenet_v2_0.pth\",\n",
    "              \"Best_IoU/upp_mobilenetv2_0.pth\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: After a round of pseudo-labeling, one should also incorporate the latest fine-tuned model (obtained by running the `src/train_pseudo_label.py` script) in the ensemble for better results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_single(model_def, weights, dir_path, conf_thres=0.95, pixel_thres=0.9):\n",
    "    models = []\n",
    "    for model_def, weight in zip(model_defs, weights):\n",
    "        model_def.load_state_dict(torch.load(weight))\n",
    "        model = tta.SegmentationTTAWrapper(model_def, tta.aliases.d4_transform(), merge_mode=\"mean\") \n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            model = nn.DataParallel(model)\n",
    "        models.append(model)\n",
    "    \n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "    \n",
    "    vv_s = []\n",
    "    vh_s = []\n",
    "    masks = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader):\n",
    "            # load image and mask into device memory\n",
    "            image = batch['image'].to(device)\n",
    "\n",
    "            # pass images into model\n",
    "            preds = []\n",
    "            for model in models:\n",
    "                pred = model(image)\n",
    "                preds.append(pred.detach().cpu().numpy())\n",
    "            \n",
    "            preds = np.array(preds)\n",
    "            preds = np.mean(preds, axis=0) # Mean over ensembles\n",
    "            \n",
    "            filter_preds, _ = nn.Softmax(dim=1)(torch.tensor(preds)).max(1) # Apply softmax -> take max\n",
    "            # Shape: (batch_size, 256, 256)\n",
    "            filter_preds = filter_preds.numpy()\n",
    "            \n",
    "            # Are `pixel_thres`% of the pixels in an entry greater `conf_thres`? \n",
    "            filerted = np.sum(filter_preds > conf_thres, axis = (1, 2)) > pixel_thres * 256 * 256 # 256: image size\n",
    "\n",
    "            for idx, filter_ in enumerate(filerted): # entries: (batch_size, 256, 256)\n",
    "                if filter_:\n",
    "                    vv_s.append(batch['vv_image_path'][idx])\n",
    "                    vh_s.append(batch['vh_image_path'][idx])\n",
    "                    entry = nn.Softmax(dim=0)(torch.tensor(preds[idx])).argmax(0).numpy() * 255. \n",
    "                    \n",
    "                    pseudo_path = \"_\".join(batch['vv_image_path'][idx].split(\"/\")[-1].split(\"_\")[:-1]) + \".png\"\n",
    "                    pseudo_path = os.path.join(dir_path, pseudo_path)\n",
    "                    masks.append(pseudo_path)\n",
    "                    cv2.imwrite(pseudo_path, entry.astype(\"float32\"))\n",
    "                    \n",
    "    return vv_s, vh_s, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vv_s, vh_s, masks =  get_predictions_single(model_defs, model_paths, \"pseudo_labels\")\n",
    "assert len(vv_s) == len(vh_s) == len(masks)\n",
    "\n",
    "paths = {'vv_image_path': vv_s,\n",
    "         'vh_image_path': vh_s,\n",
    "         'flood_label_path': masks\n",
    "}\n",
    "\n",
    "pseudo_df = pd.DataFrame(paths)\n",
    "print(pseudo_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_df.to_csv(\"pseudo_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataframe is then used to retrain any of the model used in the ensemble here (refer to `src/train_pseudo_label.py`). "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ECTI_Jumpstart.ipynb",
   "provenance": []
  },
  "environment": {
   "name": "tf2-gpu.2-4.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m65"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
